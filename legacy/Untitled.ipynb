{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-79f5e786a2da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim, logging\n",
    "import csv, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load(\"patents.wv\", mmap='r')\n",
    "train_arr = np.empty((100,))\n",
    "train_labels = np.array([])\n",
    "ret = []\n",
    "ret_labels = []\n",
    "def get_sentence_vector(sentence):\n",
    "    ret = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            ret.append(wv[porter.stem(word)])\n",
    "        except:\n",
    "            pass\n",
    "    return np.mean(ret, axis=0)\n",
    "with open('../test_dataset.csv') as csvfile:\n",
    "\tporter = PorterStemmer()\n",
    "\tfor row in csv.reader(csvfile):\n",
    "\t\ttry:\n",
    "\t\t\t#wv.most_similar(porter.stem(i), topn=1)[0][0]\n",
    "\t\t\tmean_arr = get_sentence_vector(word_tokenize(row[1]))\n",
    "\t\t\tret.append(mean_arr)\n",
    "\t\t\tret_labels.append(row[0])\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"didn't work\")\n",
    "\t\t\tpass\n",
    "print(np.vstack(ret).shape)\n",
    "try:\n",
    "\tnp.save(\"test_arr.npy\", np.vstack(ret))\n",
    "\tnp.save(\"test_arr_labels.npy\", np.vstack(ret_labels))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317974, 1000)\n",
      "(317974, 1)\n"
     ]
    }
   ],
   "source": [
    "training_arr = np.load(\"training_arr.npy\")\n",
    "print(training_arr.shape)\n",
    "training_arr_labels = np.load(\"training_arr_labels.npy\")\n",
    "print(training_arr_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section A\n",
    "    Class 01\n",
    "        Subclass B\n",
    "            Group 33\n",
    "                Main group 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.load(\"test_arr.npy\")\n",
    "print(test_arr.shape)\n",
    "test_arr_labels = np.load(\"test_arr_labels.npy\")\n",
    "\n",
    "def convert_labels_to_vector(label):\n",
    "    sections = set()\n",
    "    classes = set()\n",
    "    subclasses = set()\n",
    "    groups = set()\n",
    "    main_groups = set()\n",
    "    for z in label[0].split(','):   \n",
    "        cur = z.strip().strip('\\'\\\"')\n",
    "        sections.add(str(cur[0]))\n",
    "        classes.add(cur[1:3])\n",
    "        subclasses.add(str(cur[3]))\n",
    "        groups.add(cur.split(\" \")[1])\n",
    "    return [sections, classes,\n",
    "    subclasses,\n",
    "    groups]\n",
    "\n",
    "#test_arr_labels = np.array([convert_labels_to_vector(i) for i in test_arr_labels])\n",
    "#training_arr_labels = np.array([convert_labels_to_vector(i) for i in training_arr_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def convert_label_to_vector(label):\n",
    "    first_half, second_half = label.split(\" \")\n",
    "    return [ord(i) for i in first_half]+[sum([ord(i) for i in second_half])]\n",
    "all_labels = set([tuple(i) for i in np.load(\"label_pop.npy\")])\n",
    "training_dataset = []\n",
    "n = 0\n",
    "zed = len(training_arr_labels)\n",
    "for label, vector in zip(training_arr_labels, training_arr):\n",
    "    label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "    non_true_vectors = []\n",
    "    while len(non_true_vectors) <= len(label_vectors):\n",
    "        cur = random.sample(all_labels, 1)\n",
    "        if cur not in label_vectors:\n",
    "            non_true_vectors.append(cur)\n",
    "    labels = [True for _ in label_vectors]+[False for _ in non_true_vectors]\n",
    "    left = np.vstack((np.vstack(label_vectors), np.vstack(non_true_vectors)))\n",
    "    right = np.vstack([vector]*len(label_vectors+non_true_vectors))\n",
    "    for l, r, label in zip(left, right, labels):\n",
    "        training_dataset.append([label, np.hstack((l, r))])\n",
    "    print(float(n)/zed)\n",
    "    n=n+1\n",
    "\n",
    "print(training_dataset)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.array(training_dataset, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load(\"training_dataset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    l = len(training_vectors)\n",
    "    n = 0\n",
    "    while n < l:\n",
    "        yield (training_vectors[n].reshape((1, 1005)), [training_labels[n]])\n",
    "        n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set([tuple(i) for i in np.load(\"label_pop.npy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def convert_label_to_vector(label):\n",
    "    first_half, second_half = label.split(\" \")\n",
    "    return [ord(i) for i in first_half]+[sum([ord(i) for i in second_half])]\n",
    "\n",
    "def data_generator():\n",
    "    n = 0\n",
    "    for label, vector in zip(training_arr_labels, training_arr):\n",
    "        label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "        non_true_vectors = []\n",
    "        while len(non_true_vectors) <= len(label_vectors):\n",
    "            cur = random.sample(all_labels, 1)\n",
    "            non_true_vectors.append(cur)\n",
    "        labels = [1 for _ in label_vectors]+[0 for _ in non_true_vectors]\n",
    "        left = np.vstack((np.vstack(label_vectors), np.vstack(non_true_vectors)))\n",
    "        right = np.vstack([vector]*len(label_vectors+non_true_vectors))\n",
    "        for l, r, label in zip(left, right, labels):\n",
    "            ret = (np.hstack((l, r)).reshape((1, 1005)), [label])\n",
    "            yield ret\n",
    "def data_generator_test():\n",
    "    n = 0\n",
    "    for label, vector in zip(testing_arr_labels, testing_arr):\n",
    "        label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "        non_true_vectors = []\n",
    "        while len(non_true_vectors) <= len(label_vectors):\n",
    "            cur = random.sample(all_labels, 1)\n",
    "            non_true_vectors.append(cur)\n",
    "        labels = [1 for _ in label_vectors]+[0 for _ in non_true_vectors]\n",
    "        left = np.vstack((np.vstack(label_vectors), np.vstack(non_true_vectors)))\n",
    "        right = np.vstack([vector]*len(label_vectors+non_true_vectors))\n",
    "        for l, r, label in zip(left, right, labels):\n",
    "            ret = (np.hstack((l, r)).reshape((1, 1005)), [label])\n",
    "            yield ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: ((160, 1, 1005), (160, 1)), types: (tf.int64, tf.int64)>\n",
      "<RepeatDataset shapes: ((160, 1, 1005), (160, 1)), types: (tf.int64, tf.int64)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ((160, 1, 1005), (160, 1)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, (tf.int64, tf.int64), (tf.TensorShape([1, 1005]), tf.TensorShape([1])))\n",
    "test_dataset = tf.data.Dataset.from_generator(data_generator_test, (tf.int64, tf.int64), (tf.TensorShape([1, 1005]), tf.TensorShape([1])))\n",
    "test_dataset = test_dataset.batch(160, drop_remainder=True).repeat()\n",
    "train_dataset = dataset.batch(160, drop_remainder=True).repeat()\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "train_dataset.cache(filename='cached_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, None, 100)         100600    \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 281,601\n",
      "Trainable params: 281,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensors((training_vectors[:300000], training_labels[:300000])), tf.data.Dataset.from_tensors((training_vectors[300000:], training_labels[300000:]))))\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(None, 1005)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, dropout=.2)),\n",
    "    tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 1086s 271ms/step - loss: 0.6931 - acc: 0.5184 - val_loss: 0.6923 - val_acc: 0.5220\n",
      "Epoch 2/20\n",
      " 938/4000 [======>.......................] - ETA: 11:46 - loss: 0.6923 - acc: 0.5222"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "history = model.fit(train_dataset, epochs=20,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=600,\n",
    "                   steps_per_epoch=4000)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset, steps=10)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7215, 1000)\n",
      "(7215, 1)\n"
     ]
    }
   ],
   "source": [
    "testing_arr = np.load(\"test_arr.npy\")\n",
    "print(testing_arr.shape)\n",
    "testing_arr_labels = np.load(\"test_arr_labels.npy\")\n",
    "print(testing_arr_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48638, 1005)\n",
      "[[0.4555314]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "label_len = len(all_labels)\n",
    "left = np.vstack(all_labels)\n",
    "for label, vector in zip(testing_arr_labels, testing_arr):\n",
    "        label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "        right = np.vstack([vector]*label_len)\n",
    "        combo = np.hstack((left, right)).reshape((label_len, 1005))\n",
    "        combo = combo[newaxis, :]\n",
    "        print(combo.shape)\n",
    "        predictions = model.predict(combo, batch_size=10)\n",
    "        print(predictions)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set([tuple(i) for i in np.load(\"label_pop.npy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45553154],\n",
       "       [0.45553154],\n",
       "       [0.45553154],\n",
       "       ...,\n",
       "       [0.45553154],\n",
       "       [0.45553154],\n",
       "       [0.45553154]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
