{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79f5e786a2da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim, logging\n",
    "import csv, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load(\"patents.wv\", mmap='r')\n",
    "train_arr = np.empty((100,))\n",
    "train_labels = np.array([])\n",
    "ret = []\n",
    "ret_labels = []\n",
    "def get_sentence_vector(sentence):\n",
    "    ret = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            ret.append(wv[porter.stem(word)])\n",
    "        except:\n",
    "            pass\n",
    "    return np.mean(ret, axis=0)\n",
    "with open('../test_dataset.csv') as csvfile:\n",
    "\tporter = PorterStemmer()\n",
    "\tfor row in csv.reader(csvfile):\n",
    "\t\ttry:\n",
    "\t\t\t#wv.most_similar(porter.stem(i), topn=1)[0][0]\n",
    "\t\t\tmean_arr = get_sentence_vector(word_tokenize(row[1]))\n",
    "\t\t\tret.append(mean_arr)\n",
    "\t\t\tret_labels.append(row[0])\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"didn't work\")\n",
    "\t\t\tpass\n",
    "print(np.vstack(ret).shape)\n",
    "try:\n",
    "\tnp.save(\"test_arr.npy\", np.vstack(ret))\n",
    "\tnp.save(\"test_arr_labels.npy\", np.vstack(ret_labels))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317974, 1000)\n",
      "(317974, 1)\n"
     ]
    }
   ],
   "source": [
    "training_arr = np.load(\"training_arr.npy\")\n",
    "print(training_arr.shape)\n",
    "training_arr_labels = np.load(\"training_arr_labels.npy\")\n",
    "print(training_arr_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section A\n",
    "    Class 01\n",
    "        Subclass B\n",
    "            Group 33\n",
    "                Main group 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7215, 1000)\n"
     ]
    }
   ],
   "source": [
    "test_arr = np.load(\"test_arr.npy\")\n",
    "print(test_arr.shape)\n",
    "test_arr_labels = np.load(\"test_arr_labels.npy\")\n",
    "\n",
    "def convert_labels_to_vector(label):\n",
    "    sections = set()\n",
    "    classes = set()\n",
    "    subclasses = set()\n",
    "    groups = set()\n",
    "    main_groups = set()\n",
    "    for z in label[0].split(','):   \n",
    "        cur = z.strip().strip('\\'\\\"')\n",
    "        sections.add(str(cur[0]))\n",
    "        classes.add(cur[1:3])\n",
    "        subclasses.add(str(cur[3]))\n",
    "        groups.add(cur.split(\" \")[1])\n",
    "    return [sections, classes,\n",
    "    subclasses,\n",
    "    groups]\n",
    "\n",
    "#test_arr_labels = np.array([convert_labels_to_vector(i) for i in test_arr_labels])\n",
    "#training_arr_labels = np.array([convert_labels_to_vector(i) for i in training_arr_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "3.14491121916e-06\n",
      "6.28982243831e-06\n",
      "9.43473365747e-06\n",
      "1.25796448766e-05\n",
      "1.57245560958e-05\n",
      "1.88694673149e-05\n",
      "2.20143785341e-05\n",
      "2.51592897533e-05\n",
      "2.83042009724e-05\n",
      "3.14491121916e-05\n",
      "3.45940234107e-05\n",
      "3.77389346299e-05\n",
      "4.0883845849e-05\n",
      "4.40287570682e-05\n",
      "4.71736682873e-05\n",
      "5.03185795065e-05\n",
      "5.34634907257e-05\n",
      "5.66084019448e-05\n",
      "5.9753313164e-05\n",
      "6.28982243831e-05\n",
      "6.60431356023e-05\n",
      "6.91880468214e-05\n",
      "7.23329580406e-05\n",
      "7.54778692598e-05\n",
      "7.86227804789e-05\n",
      "8.17676916981e-05\n",
      "8.49126029172e-05\n",
      "8.80575141364e-05\n",
      "9.12024253555e-05\n",
      "9.43473365747e-05\n",
      "9.74922477938e-05\n",
      "0.000100637159013\n",
      "0.000103782070232\n",
      "0.000106926981451\n",
      "0.00011007189267\n",
      "0.00011321680389\n",
      "0.000116361715109\n",
      "0.000119506626328\n",
      "0.000122651537547\n",
      "0.000125796448766\n",
      "0.000128941359985\n",
      "0.000132086271205\n",
      "0.000135231182424\n",
      "0.000138376093643\n",
      "0.000141521004862\n",
      "0.000144665916081\n",
      "0.0001478108273\n",
      "0.00015095573852\n",
      "0.000154100649739\n",
      "0.000157245560958\n",
      "0.000160390472177\n",
      "0.000163535383396\n",
      "0.000166680294615\n",
      "0.000169825205834\n",
      "0.000172970117054\n",
      "0.000176115028273\n",
      "0.000179259939492\n",
      "0.000182404850711\n",
      "0.00018554976193\n",
      "0.000188694673149\n",
      "0.000191839584369\n",
      "0.000194984495588\n",
      "0.000198129406807\n",
      "0.000201274318026\n",
      "0.000204419229245\n",
      "0.000207564140464\n",
      "0.000210709051683\n",
      "0.000213853962903\n",
      "0.000216998874122\n",
      "0.000220143785341\n",
      "0.00022328869656\n",
      "0.000226433607779\n",
      "0.000229578518998\n",
      "0.000232723430218\n",
      "0.000235868341437\n",
      "0.000239013252656\n",
      "0.000242158163875\n",
      "0.000245303075094\n",
      "0.000248447986313\n",
      "0.000251592897533\n",
      "0.000254737808752\n",
      "0.000257882719971\n",
      "0.00026102763119\n",
      "0.000264172542409\n",
      "0.000267317453628\n",
      "0.000270462364847\n",
      "0.000273607276067\n",
      "0.000276752187286\n",
      "0.000279897098505\n",
      "0.000283042009724\n",
      "0.000286186920943\n",
      "0.000289331832162\n",
      "0.000292476743382\n",
      "0.000295621654601\n",
      "0.00029876656582\n",
      "0.000301911477039\n",
      "0.000305056388258\n",
      "0.000308201299477\n",
      "0.000311346210696\n",
      "0.000314491121916\n",
      "0.000317636033135\n",
      "0.000320780944354\n",
      "0.000323925855573\n",
      "0.000327070766792\n",
      "0.000330215678011\n",
      "0.000333360589231\n",
      "0.00033650550045\n",
      "0.000339650411669\n",
      "0.000342795322888\n",
      "0.000345940234107\n",
      "0.000349085145326\n",
      "0.000352230056546\n",
      "0.000355374967765\n",
      "0.000358519878984\n",
      "0.000361664790203\n",
      "0.000364809701422\n",
      "0.000367954612641\n",
      "0.00037109952386\n",
      "0.00037424443508\n",
      "0.000377389346299\n",
      "0.000380534257518\n",
      "0.000383679168737\n",
      "0.000386824079956\n",
      "0.000389968991175\n",
      "0.000393113902395\n",
      "0.000396258813614\n",
      "0.000399403724833\n",
      "0.000402548636052\n",
      "0.000405693547271\n",
      "0.00040883845849\n",
      "0.000411983369709\n",
      "0.000415128280929\n",
      "0.000418273192148\n",
      "0.000421418103367\n",
      "0.000424563014586\n",
      "0.000427707925805\n",
      "0.000430852837024\n",
      "0.000433997748244\n",
      "0.000437142659463\n",
      "0.000440287570682\n",
      "0.000443432481901\n",
      "0.00044657739312\n",
      "0.000449722304339\n",
      "0.000452867215559\n",
      "0.000456012126778\n",
      "0.000459157037997\n",
      "0.000462301949216\n",
      "0.000465446860435\n",
      "0.000468591771654\n",
      "0.000471736682873\n",
      "0.000474881594093\n",
      "0.000478026505312\n",
      "0.000481171416531\n",
      "0.00048431632775\n",
      "0.000487461238969\n",
      "0.000490606150188\n",
      "0.000493751061408\n",
      "0.000496895972627\n",
      "0.000500040883846\n",
      "0.000503185795065\n",
      "0.000506330706284\n",
      "0.000509475617503\n",
      "0.000512620528722\n",
      "0.000515765439942\n",
      "0.000518910351161\n",
      "0.00052205526238\n",
      "0.000525200173599\n",
      "0.000528345084818\n",
      "0.000531489996037\n",
      "0.000534634907257\n",
      "0.000537779818476\n",
      "0.000540924729695\n",
      "0.000544069640914\n",
      "0.000547214552133\n",
      "0.000550359463352\n",
      "0.000553504374572\n",
      "0.000556649285791\n",
      "0.00055979419701\n",
      "0.000562939108229\n",
      "0.000566084019448\n",
      "0.000569228930667\n",
      "0.000572373841886\n",
      "0.000575518753106\n",
      "0.000578663664325\n",
      "0.000581808575544\n",
      "0.000584953486763\n",
      "0.000588098397982\n",
      "0.000591243309201\n",
      "0.000594388220421\n",
      "0.00059753313164\n",
      "0.000600678042859\n",
      "0.000603822954078\n",
      "0.000606967865297\n",
      "0.000610112776516\n",
      "0.000613257687735\n",
      "0.000616402598955\n",
      "0.000619547510174\n",
      "0.000622692421393\n",
      "0.000625837332612\n",
      "0.000628982243831\n",
      "0.00063212715505\n",
      "0.00063527206627\n",
      "0.000638416977489\n",
      "0.000641561888708\n",
      "0.000644706799927\n",
      "0.000647851711146\n",
      "0.000650996622365\n",
      "0.000654141533585\n",
      "0.000657286444804\n",
      "0.000660431356023\n",
      "0.000663576267242\n",
      "0.000666721178461\n",
      "0.00066986608968\n",
      "0.000673011000899\n",
      "0.000676155912119\n",
      "0.000679300823338\n",
      "0.000682445734557\n",
      "0.000685590645776\n",
      "0.000688735556995\n",
      "0.000691880468214\n",
      "0.000695025379434\n",
      "0.000698170290653\n",
      "0.000701315201872\n",
      "0.000704460113091\n",
      "0.00070760502431\n",
      "0.000710749935529\n",
      "0.000713894846748\n",
      "0.000717039757968\n",
      "0.000720184669187\n",
      "0.000723329580406\n",
      "0.000726474491625\n",
      "0.000729619402844\n",
      "0.000732764314063\n",
      "0.000735909225283\n",
      "0.000739054136502\n",
      "0.000742199047721\n",
      "0.00074534395894\n",
      "0.000748488870159\n",
      "0.000751633781378\n",
      "0.000754778692598\n",
      "0.000757923603817\n",
      "0.000761068515036\n",
      "0.000764213426255\n",
      "0.000767358337474\n",
      "0.000770503248693\n",
      "0.000773648159912\n",
      "0.000776793071132\n",
      "0.000779937982351\n",
      "0.00078308289357\n",
      "0.000786227804789\n",
      "0.000789372716008\n",
      "0.000792517627227\n",
      "0.000795662538447\n",
      "0.000798807449666\n",
      "0.000801952360885\n",
      "0.000805097272104\n",
      "0.000808242183323\n",
      "0.000811387094542\n",
      "0.000814532005761\n",
      "0.000817676916981\n",
      "0.0008208218282\n",
      "0.000823966739419\n",
      "0.000827111650638\n",
      "0.000830256561857\n",
      "0.000833401473076\n",
      "0.000836546384296\n",
      "0.000839691295515\n",
      "0.000842836206734\n",
      "0.000845981117953\n",
      "0.000849126029172\n",
      "0.000852270940391\n",
      "0.000855415851611\n",
      "0.00085856076283\n",
      "0.000861705674049\n",
      "0.000864850585268\n",
      "0.000867995496487\n",
      "0.000871140407706\n",
      "0.000874285318925\n",
      "0.000877430230145\n",
      "0.000880575141364\n",
      "0.000883720052583\n",
      "0.000886864963802\n",
      "0.000890009875021\n",
      "0.00089315478624\n",
      "0.00089629969746\n",
      "0.000899444608679\n",
      "0.000902589519898\n",
      "0.000905734431117\n",
      "0.000908879342336\n",
      "0.000912024253555\n",
      "0.000915169164774\n",
      "0.000918314075994\n",
      "0.000921458987213\n",
      "0.000924603898432\n",
      "0.000927748809651\n",
      "0.00093089372087\n",
      "0.000934038632089\n",
      "0.000937183543309\n",
      "0.000940328454528\n",
      "0.000943473365747\n",
      "0.000946618276966\n",
      "0.000949763188185\n",
      "0.000952908099404\n",
      "0.000956053010624\n",
      "0.000959197921843\n",
      "0.000962342833062\n",
      "0.000965487744281\n",
      "0.0009686326555\n",
      "0.000971777566719\n",
      "0.000974922477938\n",
      "0.000978067389158\n",
      "0.000981212300377\n",
      "0.000984357211596\n",
      "0.000987502122815\n",
      "0.000990647034034\n",
      "0.000993791945253\n",
      "0.000996936856473\n",
      "0.00100008176769\n",
      "0.00100322667891\n",
      "0.00100637159013\n",
      "0.00100951650135\n",
      "0.00101266141257\n",
      "0.00101580632379\n",
      "0.00101895123501\n",
      "0.00102209614623\n",
      "0.00102524105744\n",
      "0.00102838596866\n",
      "0.00103153087988\n",
      "0.0010346757911\n",
      "0.00103782070232\n",
      "0.00104096561354\n",
      "0.00104411052476\n",
      "0.00104725543598\n",
      "0.0010504003472\n",
      "0.00105354525842\n",
      "0.00105669016964\n",
      "0.00105983508086\n",
      "0.00106297999207\n",
      "0.00106612490329\n",
      "0.00106926981451\n",
      "0.00107241472573\n",
      "0.00107555963695\n",
      "0.00107870454817\n",
      "0.00108184945939\n",
      "0.00108499437061\n",
      "0.00108813928183\n",
      "0.00109128419305\n",
      "0.00109442910427\n",
      "0.00109757401549\n",
      "0.0011007189267\n",
      "0.00110386383792\n",
      "0.00110700874914\n",
      "0.00111015366036\n",
      "0.00111329857158\n",
      "0.0011164434828\n",
      "0.00111958839402\n",
      "0.00112273330524\n",
      "0.00112587821646\n",
      "0.00112902312768\n",
      "0.0011321680389\n",
      "0.00113531295012\n",
      "0.00113845786133\n",
      "0.00114160277255\n",
      "0.00114474768377\n",
      "0.00114789259499\n",
      "0.00115103750621\n",
      "0.00115418241743\n",
      "0.00115732732865\n",
      "0.00116047223987\n",
      "0.00116361715109\n",
      "0.00116676206231\n",
      "0.00116990697353\n",
      "0.00117305188475\n",
      "0.00117619679596\n",
      "0.00117934170718\n",
      "0.0011824866184\n",
      "0.00118563152962\n",
      "0.00118877644084\n",
      "0.00119192135206\n",
      "0.00119506626328\n",
      "0.0011982111745\n",
      "0.00120135608572\n",
      "0.00120450099694\n",
      "0.00120764590816\n",
      "0.00121079081938\n",
      "0.00121393573059\n",
      "0.00121708064181\n",
      "0.00122022555303\n",
      "0.00122337046425\n",
      "0.00122651537547\n",
      "0.00122966028669\n",
      "0.00123280519791\n",
      "0.00123595010913\n",
      "0.00123909502035\n",
      "0.00124223993157\n",
      "0.00124538484279\n",
      "0.00124852975401\n",
      "0.00125167466522\n",
      "0.00125481957644\n",
      "0.00125796448766\n",
      "0.00126110939888\n",
      "0.0012642543101\n",
      "0.00126739922132\n",
      "0.00127054413254\n",
      "0.00127368904376\n",
      "0.00127683395498\n",
      "0.0012799788662\n",
      "0.00128312377742\n",
      "0.00128626868863\n",
      "0.00128941359985\n",
      "0.00129255851107\n",
      "0.00129570342229\n",
      "0.00129884833351\n",
      "0.00130199324473\n",
      "0.00130513815595\n",
      "0.00130828306717\n",
      "0.00131142797839\n",
      "0.00131457288961\n",
      "0.00131771780083\n",
      "0.00132086271205\n",
      "0.00132400762326\n",
      "0.00132715253448\n",
      "0.0013302974457\n",
      "0.00133344235692\n",
      "0.00133658726814\n",
      "0.00133973217936\n",
      "0.00134287709058\n",
      "0.0013460220018\n",
      "0.00134916691302\n",
      "0.00135231182424\n",
      "0.00135545673546\n",
      "0.00135860164668\n",
      "0.00136174655789\n",
      "0.00136489146911\n",
      "0.00136803638033\n",
      "0.00137118129155\n",
      "0.00137432620277\n",
      "0.00137747111399\n",
      "0.00138061602521\n",
      "0.00138376093643\n",
      "0.00138690584765\n",
      "0.00139005075887\n",
      "0.00139319567009\n",
      "0.00139634058131\n",
      "0.00139948549252\n",
      "0.00140263040374\n",
      "0.00140577531496\n",
      "0.00140892022618\n",
      "0.0014120651374\n",
      "0.00141521004862\n",
      "0.00141835495984\n",
      "0.00142149987106\n",
      "0.00142464478228\n",
      "0.0014277896935\n",
      "0.00143093460472\n",
      "0.00143407951594\n",
      "0.00143722442715\n",
      "0.00144036933837\n",
      "0.00144351424959\n",
      "0.00144665916081\n",
      "0.00144980407203\n",
      "0.00145294898325\n",
      "0.00145609389447\n",
      "0.00145923880569\n",
      "0.00146238371691\n",
      "0.00146552862813\n",
      "0.00146867353935\n",
      "0.00147181845057\n",
      "0.00147496336178\n",
      "0.001478108273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00148125318422\n",
      "0.00148439809544\n",
      "0.00148754300666\n",
      "0.00149068791788\n",
      "0.0014938328291\n",
      "0.00149697774032\n",
      "0.00150012265154\n",
      "0.00150326756276\n",
      "0.00150641247398\n",
      "0.0015095573852\n",
      "0.00151270229641\n",
      "0.00151584720763\n",
      "0.00151899211885\n",
      "0.00152213703007\n",
      "0.00152528194129\n",
      "0.00152842685251\n",
      "0.00153157176373\n",
      "0.00153471667495\n",
      "0.00153786158617\n",
      "0.00154100649739\n",
      "0.00154415140861\n",
      "0.00154729631982\n",
      "0.00155044123104\n",
      "0.00155358614226\n",
      "0.00155673105348\n",
      "0.0015598759647\n",
      "0.00156302087592\n",
      "0.00156616578714\n",
      "0.00156931069836\n",
      "0.00157245560958\n",
      "0.0015756005208\n",
      "0.00157874543202\n",
      "0.00158189034324\n",
      "0.00158503525445\n",
      "0.00158818016567\n",
      "0.00159132507689\n",
      "0.00159446998811\n",
      "0.00159761489933\n",
      "0.00160075981055\n",
      "0.00160390472177\n",
      "0.00160704963299\n",
      "0.00161019454421\n",
      "0.00161333945543\n",
      "0.00161648436665\n",
      "0.00161962927787\n",
      "0.00162277418908\n",
      "0.0016259191003\n",
      "0.00162906401152\n",
      "0.00163220892274\n",
      "0.00163535383396\n",
      "0.00163849874518\n",
      "0.0016416436564\n",
      "0.00164478856762\n",
      "0.00164793347884\n",
      "0.00165107839006\n",
      "0.00165422330128\n",
      "0.0016573682125\n",
      "0.00166051312371\n",
      "0.00166365803493\n",
      "0.00166680294615\n",
      "0.00166994785737\n",
      "0.00167309276859\n",
      "0.00167623767981\n",
      "0.00167938259103\n",
      "0.00168252750225\n",
      "0.00168567241347\n",
      "0.00168881732469\n",
      "0.00169196223591\n",
      "0.00169510714713\n",
      "0.00169825205834\n",
      "0.00170139696956\n",
      "0.00170454188078\n",
      "0.001707686792\n",
      "0.00171083170322\n",
      "0.00171397661444\n",
      "0.00171712152566\n",
      "0.00172026643688\n",
      "0.0017234113481\n",
      "0.00172655625932\n",
      "0.00172970117054\n",
      "0.00173284608176\n",
      "0.00173599099297\n",
      "0.00173913590419\n",
      "0.00174228081541\n",
      "0.00174542572663\n",
      "0.00174857063785\n",
      "0.00175171554907\n",
      "0.00175486046029\n",
      "0.00175800537151\n",
      "0.00176115028273\n",
      "0.00176429519395\n",
      "0.00176744010517\n",
      "0.00177058501638\n",
      "0.0017737299276\n",
      "0.00177687483882\n",
      "0.00178001975004\n",
      "0.00178316466126\n",
      "0.00178630957248\n",
      "0.0017894544837\n",
      "0.00179259939492\n",
      "0.00179574430614\n",
      "0.00179888921736\n",
      "0.00180203412858\n",
      "0.0018051790398\n",
      "0.00180832395101\n",
      "0.00181146886223\n",
      "0.00181461377345\n",
      "0.00181775868467\n",
      "0.00182090359589\n",
      "0.00182404850711\n",
      "0.00182719341833\n",
      "0.00183033832955\n",
      "0.00183348324077\n",
      "0.00183662815199\n",
      "0.00183977306321\n",
      "0.00184291797443\n",
      "0.00184606288564\n",
      "0.00184920779686\n",
      "0.00185235270808\n",
      "0.0018554976193\n",
      "0.00185864253052\n",
      "0.00186178744174\n",
      "0.00186493235296\n",
      "0.00186807726418\n",
      "0.0018712221754\n",
      "0.00187436708662\n",
      "0.00187751199784\n",
      "0.00188065690906\n",
      "0.00188380182027\n",
      "0.00188694673149\n",
      "0.00189009164271\n",
      "0.00189323655393\n",
      "0.00189638146515\n",
      "0.00189952637637\n",
      "0.00190267128759\n",
      "0.00190581619881\n",
      "0.00190896111003\n",
      "0.00191210602125\n",
      "0.00191525093247\n",
      "0.00191839584369\n",
      "0.0019215407549\n",
      "0.00192468566612\n",
      "0.00192783057734\n",
      "0.00193097548856\n",
      "0.00193412039978\n",
      "0.001937265311\n",
      "0.00194041022222\n",
      "0.00194355513344\n",
      "0.00194670004466\n",
      "0.00194984495588\n",
      "0.0019529898671\n",
      "0.00195613477832\n",
      "0.00195927968953\n",
      "0.00196242460075\n",
      "0.00196556951197\n",
      "0.00196871442319\n",
      "0.00197185933441\n",
      "0.00197500424563\n",
      "0.00197814915685\n",
      "0.00198129406807\n",
      "0.00198443897929\n",
      "0.00198758389051\n",
      "0.00199072880173\n",
      "0.00199387371295\n",
      "0.00199701862416\n",
      "0.00200016353538\n",
      "0.0020033084466\n",
      "0.00200645335782\n",
      "0.00200959826904\n",
      "0.00201274318026\n",
      "0.00201588809148\n",
      "0.0020190330027\n",
      "0.00202217791392\n",
      "0.00202532282514\n",
      "0.00202846773636\n",
      "0.00203161264757\n",
      "0.00203475755879\n",
      "0.00203790247001\n",
      "0.00204104738123\n",
      "0.00204419229245\n",
      "0.00204733720367\n",
      "0.00205048211489\n",
      "0.00205362702611\n",
      "0.00205677193733\n",
      "0.00205991684855\n",
      "0.00206306175977\n",
      "0.00206620667099\n",
      "0.0020693515822\n",
      "0.00207249649342\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7e6236625792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnon_true_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTrue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_true_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_true_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_vectors\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnon_true_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \"\"\"\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Don't run this\n",
    "\n",
    "import random\n",
    "def convert_label_to_vector(label):\n",
    "    first_half, second_half = label.split(\" \")\n",
    "    return [ord(i) for i in first_half]+[sum([ord(i) for i in second_half])]\n",
    "all_labels = set([tuple(i) for i in np.load(\"label_pop.npy\")])\n",
    "training_dataset = []\n",
    "n = 0\n",
    "zed = len(training_arr_labels)\n",
    "for label, vector in zip(training_arr_labels, training_arr):\n",
    "    label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "    non_true_vectors = []\n",
    "    while len(non_true_vectors) <= len(label_vectors):\n",
    "        cur = random.sample(all_labels, 1)\n",
    "        if cur not in label_vectors:\n",
    "            non_true_vectors.append(cur)\n",
    "    labels = [True for _ in label_vectors]+[False for _ in non_true_vectors]\n",
    "    left = np.vstack((np.vstack(label_vectors), np.vstack(non_true_vectors)))\n",
    "    right = np.vstack([vector]*len(label_vectors+non_true_vectors))\n",
    "    for l, r, label in zip(left, right, labels):\n",
    "        training_dataset.append([label, np.hstack((l, r))])\n",
    "    print(float(n)/zed)\n",
    "    n=n+1\n",
    "\n",
    "print(training_dataset)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.array(training_dataset, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load(\"training_dataset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this either\n",
    "\n",
    "def data_generator():\n",
    "    l = len(training_vectors)\n",
    "    n = 0\n",
    "    while n < l:\n",
    "        yield (training_vectors[n].reshape((1, 1005)), [training_labels[n]])\n",
    "        n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set([tuple(i) for i in np.load(\"label_pop.npy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def convert_label_to_vector(label):\n",
    "    first_half, second_half = label.split(\" \")\n",
    "    return [ord(i) for i in first_half]+[sum([ord(i) for i in second_half])]\n",
    "\n",
    "def data_generator():\n",
    "    n = 0\n",
    "    for label, vector in zip(training_arr_labels, training_arr):\n",
    "        label_vectors = [convert_label_to_vector(z.strip().strip('\\'\\\"')) for z in label[0].split(',')]\n",
    "        non_true_vectors = []\n",
    "        while len(non_true_vectors) <= len(label_vectors):\n",
    "            cur = random.sample(all_labels, 1)\n",
    "            non_true_vectors.append(cur)\n",
    "        labels = [True for _ in label_vectors]+[False for _ in non_true_vectors]\n",
    "        left = np.vstack((np.vstack(label_vectors), np.vstack(non_true_vectors)))\n",
    "        right = np.vstack([vector]*len(label_vectors+non_true_vectors))\n",
    "        for l, r, label in zip(left, right, labels):\n",
    "            ret = (np.hstack((l, r)).reshape((1, 1005)), [label])\n",
    "            yield ret\n",
    "        n=n+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: ((5000, 1, 1005), (5000, 1)), types: (tf.int64, tf.bool)>\n",
      "<RepeatDataset shapes: ((1000, 1, 1005), (1000, 1)), types: (tf.int64, tf.bool)>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 1005)        1011030   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100)               422400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,433,531\n",
      "Trainable params: 1,433,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, (tf.int64, tf.bool), (tf.TensorShape([1, 1005]), tf.TensorShape([1])))\n",
    "test_dataset = dataset.take(1000000) \n",
    "test_dataset = test_dataset.batch(1000, drop_remainder=True).repeat()\n",
    "train_dataset = dataset.skip(1000000)\n",
    "train_dataset = train_dataset.batch(5000, drop_remainder=True).repeat()\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "#train_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensors((training_vectors[:300000], training_labels[:300000])), tf.data.Dataset.from_tensors((training_vectors[300000:], training_labels[300000:]))))\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1005, activation='sigmoid', input_shape=(None, 1005)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n",
    "    tf.keras.layers.Dense(1, activation='relu'),\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=100,\n",
    "                   steps_per_epoch=100000)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset, steps=10)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
